{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in /Users/prabhatturlapati/opt/anaconda3/lib/python3.7/site-packages (2.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import dask.dataframe as dd\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('../data/quora-question-pairs/train.csv.zip').dropna()\n",
    "test = pd.read_csv('../data/quora-question-pairs/test.csv').dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the words in the training data\n",
    "\n",
    "# initialize dataframe with question and qid\n",
    "qid1 = train['qid1'].values.tolist()\n",
    "qid2 = train['qid2'].values.tolist()\n",
    "question1 = train['question1'].values.tolist()\n",
    "question2 = train['question2'].values.tolist()\n",
    "qid_quest1 = pd.DataFrame.from_dict({'qid':qid1, 'question':question1})\n",
    "qid_quest2 = pd.DataFrame.from_dict({'qid':qid2, 'question':question2})\n",
    "qid_quest = pd.concat([qid_quest1,qid_quest2])\n",
    "\n",
    "# collect all questions and count vectorize them\n",
    "questions = qid_quest['question'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 808574 entries, 0 to 404286\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   qid       808574 non-null  int64 \n",
      " 1   question  808574 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 18.5+ MB\n"
     ]
    }
   ],
   "source": [
    "qid_quest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/prabhatturlapati/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prabhatturlapati/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/prabhatturlapati/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the step by step guide to invest in share market in india? \n",
      " ['What', 'step', 'step', 'guide', 'invest', 'share', 'market', 'india']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "lemmatizer = WordNetLemmatizer() # Word net lemmatizer is pretty elaborate in its approach read this : https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    \n",
    "def remove_stopwords (sentence, stopwords_list):\n",
    "    \"\"\"This method removes stop words from an untokenized/tokenized sentence and returns a token of strings\"\"\"\n",
    "    final_tokens = []\n",
    "    for each_word in sentence:\n",
    "        if each_word not in stopwords_list:\n",
    "            final_tokens.append(each_word)\n",
    "    return final_tokens\n",
    "\n",
    "def get_part_of_speech(sentence):\n",
    "    \"\"\"This method gets the part of speech in context of the sentence the word is in, \n",
    "    this is better than using the vanilla lemmatizer without any part of speech tag. \n",
    "    Also this returns Wordnet tags which can directly\n",
    "    be used in the wordnet lemmatizer. Use this before removing stop words. Note this is NLTK specific\"\"\"\n",
    "    tagged_sentence = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    return tagged_sentence\n",
    "\n",
    "def convert_pos_to_wordnet(word, nltk_tag):\n",
    "    \"\"\"This method converts nltk tag to wordnet tag\"\"\"\n",
    "    wn_pos = None\n",
    "    if nltk_tag.startswith('J'):\n",
    "        wn_pos = wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        wn_pos = wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        wn_pos = wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        wn_pos = wordnet.ADV\n",
    "    else:          \n",
    "        wn_pos = wordnet.NOUN\n",
    "    return word, wn_pos\n",
    "\n",
    "    \n",
    "def lemmatize_sentence(sentence, lemmatizer=lemmatizer):\n",
    "    \"\"\"This method uses an nltk lemmatizer on a sentence\"\"\"\n",
    "    wn_tagged_sent = [lemmatizer.lemmatize(convert_pos_to_wordnet(x[0],x[1])[0], pos=convert_pos_to_wordnet(x[0],x[1])[1]) for x in get_part_of_speech(sentence)  if not x[0] in stop_words and x[0].isalnum()]\n",
    "    return wn_tagged_sent\n",
    "\n",
    "# multiprocessing ready lemmatize\n",
    "def lemmatize_sentence_mp(sentence):\n",
    "    \"\"\"This method uses an nltk lemmatizer on a sentence\"\"\"\n",
    "    wn_tagged_sent = [convert_pos_to_wordnet(x[0],x[1]) for x in get_part_of_speech(sentence)]\n",
    "    return \" \".join([lemmatizer.lemmatize(x[0], pos=x[1]) for x in wn_tagged_sent if not x[0] in stop_words and x[0].isalnum()])\n",
    "\n",
    "test_sent = questions[0]\n",
    "print(test_sent, \"\\n\",lemmatize_sentence(test_sent,lemmatizer))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count(sentence):\n",
    "    return len(sentence.split())\n",
    "max(qid_quest['question'].apply(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = qid_quest.apply(lambda x: lemmatize_sentence(x['question']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404282</th>\n",
       "      <td>379845</td>\n",
       "      <td>How many keywords are there in PERL Programmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404283</th>\n",
       "      <td>155606</td>\n",
       "      <td>Is it true that there is life after death?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404284</th>\n",
       "      <td>537929</td>\n",
       "      <td>What's this coin?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404285</th>\n",
       "      <td>537931</td>\n",
       "      <td>I am having little hairfall problem but I want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404286</th>\n",
       "      <td>537933</td>\n",
       "      <td>What is it like to have sex with your cousin?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           qid                                           question\n",
       "0            1  What is the step by step guide to invest in sh...\n",
       "1            3  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2            5  How can I increase the speed of my internet co...\n",
       "3            7  Why am I mentally very lonely? How can I solve...\n",
       "4            9  Which one dissolve in water quikly sugar, salt...\n",
       "...        ...                                                ...\n",
       "404282  379845  How many keywords are there in PERL Programmin...\n",
       "404283  155606         Is it true that there is life after death?\n",
       "404284  537929                                  What's this coin?\n",
       "404285  537931  I am having little hairfall problem but I want...\n",
       "404286  537933      What is it like to have sex with your cousin?\n",
       "\n",
       "[808574 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid_quest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prabhatturlapati/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas.util.testing as pdt\n",
    "\n",
    "# def process_apply(x):\n",
    "#     # do some stuff to data here\n",
    "\n",
    "def process(df):\n",
    "    print(1)\n",
    "    res = df.apply(lambda x: lemmatize_sentence(x['question']), axis=1)\n",
    "    return res\n",
    "\n",
    "print(4*mp.cpu_count())\n",
    "p = mp.Pool(processes=4*mp.cpu_count())\n",
    "split_dfs = np.array_split(qid_quest,4*mp.cpu_count())\n",
    "pool_results = p.map(process, split_dfs)\n",
    "p.close()\n",
    "p.join()\n",
    "    \n",
    "    \n",
    "# merging parts processed by different processes\n",
    "parts = pd.concat(pool_results, axis=0)\n",
    "\n",
    "# # merging newly calculated parts to big_df\n",
    "# big_df = pd.concat([big_df, parts], axis=1)\n",
    "\n",
    "# # checking if the dfs were merged correctly\n",
    "# pdt.assert_series_equal(parts['id'], big_df['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dataset :  404290\n",
      "number of duplicates :  149263 ratio in dataset :  0.369197853026293\n",
      "number of non-duplicates :  255027 ratio in dataset :  0.630802146973707\n"
     ]
    }
   ],
   "source": [
    "# distribution of is_duplicate\n",
    "\n",
    "print(\"len of dataset : \", len(train))\n",
    "print(\"number of duplicates : \", len(train.loc[train['is_duplicate']==1]), \"ratio in dataset : \", len(train.loc[train['is_duplicate']==1])/len(train))\n",
    "print(\"number of non-duplicates : \", len(train.loc[train['is_duplicate']==0]), \"ratio in dataset : \", len(train.loc[train['is_duplicate']==0])/len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Doc2Vec Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize Q1 and Q2 for each row\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora\n",
    "from gensim.matutils import softcossim\n",
    "\n",
    "sent_1 = 'Dravid is a cricket player and a opening batsman'.split()\n",
    "sent_2 = 'Leo is a cricket player too He is a batsman,baller and keeper'.split()\n",
    "\n",
    "# Download the FastText model\n",
    "fasttext_model300 = api.load('glove-twitter-25')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# apply function\n",
    "def soft_cosine(sent_1, sent_2):\n",
    "    sent_1 = sent_1.split()\n",
    "    sent_2 = sent_2.split()\n",
    "    dictionary = corpora.Dictionary([sent_1, sent_2])\n",
    "    similarity_matrix = fasttext_model300.similarity_matrix(dictionary)\n",
    "    t_sent_1 = dictionary.doc2bow(sent_1)\n",
    "    t_sent_2 = dictionary.doc2bow(sent_2)\n",
    "    sc = softcossim(t_sent_1, t_sent_2, similarity_matrix)\n",
    "    return sc\n",
    "    \n",
    "def cosine(sent_1, sent_2):\n",
    "#     sent_1 = sent_1.split()\n",
    "#     sent_2 = sent_2.split()\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([sent_1, sent_2])\n",
    "    cs = cosine_similarity(X[0],X[1])\n",
    "    return cs[0][0]\n",
    "#     dictionary = corpora.Dictionary([sent_1, sent_2])\n",
    "#     similarity_matrix = fasttext_model300.similarity_matrix(dictionary)\n",
    "#     t_sent_1 = dictionary.doc2bow(sent_1)\n",
    "#     t_sent_2 = dictionary.doc2bow(sent_2)\n",
    "#     sc = softcossim(t_sent_1, t_sent_2, similarity_matrix)\n",
    "#     return sc\n",
    "cosine(train.iloc[0]['question1'], train.iloc[0]['question2'])\n",
    "    \n",
    "# train['soft_cosine']= train.apply(lambda x: soft_cosine(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = train.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len of dataset : \", len(sample_train))\n",
    "print(\"number of duplicates : \", len(sample_train.loc[sample_train['is_duplicate']==1]), \"ratio in dataset : \", len(train.loc[train['is_duplicate']==1])/len(train))\n",
    "print(\"number of non-duplicates : \", len(sample_train.loc[sample_train['is_duplicate']==0]), \"ratio in dataset : \", len(train.loc[train['is_duplicate']==0])/len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "def parallelize(data, func, num_of_processes=8):\n",
    "    data_split = np.array_split(data, num_of_processes)\n",
    "    pool = Pool(num_of_processes)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "def run_on_subset(data_subset):\n",
    "    return data_subset.apply(lambda x: soft_cosine(x['question1'], x['question2']), axis=1)\n",
    "#     return data_subset.apply(lambda x: cosine(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "def parallelize_on_rows(data, num_of_processes=8):\n",
    "    return parallelize(data, partial(run_on_subset), num_of_processes)\n",
    "\n",
    "\n",
    "df_res = parallelize_on_rows(sample_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train['soft_cos'] = df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = sample_train.loc[sample_train['is_duplicate']==1]['soft_cos']\n",
    "y = sample_train.loc[sample_train['is_duplicate']==0]['soft_cos']\n",
    "plt.hist([x, y], 10, label=['1', '0'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "print(\"Average Similarity for duplicates : \", np.mean(x))\n",
    "print(\"Average Similarity for not duplicates : \", np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample_train.loc[sample_train['is_duplicate']==1]['soft_cos']\n",
    "y = sample_train.loc[sample_train['is_duplicate']==0]['soft_cos']\n",
    "plt.boxplot([x, y])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train.loc[sample_train['is_duplicate']==0]\n",
    "threshold = 0.862\n",
    "sample_train['predicted_is_duplicate'] = np.where(sample_train['soft_cos']>threshold,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy \",accuracy_score(sample_train['is_duplicate'],sample_train['predicted_is_duplicate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using above threshold , test the test data\n",
    "test_res= parallelize_on_rows(test,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
